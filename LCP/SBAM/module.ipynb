{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Bayesian Auxiliary Module _(SBAM)_\n",
    "In this notebook we will try to explore the `PySINDy` package architecture, in order to easily implement our bayesian version of this algorithm with a sparsity inducing prior.\n",
    "\n",
    "In `PySINDy`, the main object is the `pysindy.SINDy` model object, built as a subclass of the `BaseEstimator` object from `sklearn` in order to ensure compatibility; this serves as a wrapper for all of the component necessary to the algorithm's implementation, that are\n",
    "\n",
    "- *differentiation* : the submodules tasked with the computation of the time derivatives\n",
    "- *feature_library* : the submodules tasked with the construction of the \"candidate features\" of the linear regression\n",
    "- *optimizers* : the submodules tasked with performing the linear regression\n",
    "\n",
    "For each of these tasks, several algorithms alternatives are available, therefore several different submodules can be deployed.\n",
    "\n",
    "We will implement a \"custom\" `optimizer` module, that will perform Gibbs sampling of our posterior distribution for all the random parameters defined in the model. The object will retain the samples as an attribute, and will communicate their (conditioned on the masking mode) mean as the best coefficients to the rest of the `PySINDy` package.\n",
    "\n",
    "## The `BaseOptimizer` class\n",
    "\n",
    "This is the wrapper class for each optimizer algorithm that the package provides; we will build a optimizer module as a subclass of this wrapper. <a href=https://pysindy.readthedocs.io/en/latest/_modules/pysindy/optimizers/base.html#BaseOptimizer>Source code</a> is available on the documentation. \n",
    "\n",
    "The `BaseOptimizer` class implements a `.fit()` method that is called by the `SINDy` object, feeding it the target variable (the time derivative) and the features (built with the `feature_library` module). The `.fit()` itself (after eventually some optional data manipulation such as feature normalization) calls a `._reduce()` method that is supposed to be built into the actual submodule used for optimization and should carry the work to find the regression coefficients and their \"presence\" in the model, storing them in the class' attributes `.coef_` (array of floats shaped like `(n_targets,n_features)`) and  `.ind_` (array of boolean values shaped like `._coef`).\n",
    "\n",
    "We will consider \"active\" those coefficients $\\beta_j$ whose $z_j$ is more likely to be 1 rather than 0 in the posterior distribution; that is, it is more probable for them to have some non-zero value (the slab) rather than zero (the spike). For these coefficients, we then calculate the mean only on those samples for which $z_j=1$, that is we will try to estimate $E[\\beta_j | z_j = 1]$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysindy.optimizers import BaseOptimizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils.validation import check_X_y\n",
    "from sklearn.linear_model._base import _preprocess_data\n",
    "from scipy.stats import beta,gamma,multivariate_normal,binom,mode\n",
    "from alive_progress import alive_bar # package to print the pretty progress bar. can be installed with pip install alive-progress\n",
    "from time import sleep # sleep during sampling to ease on cpu\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _rescale_data(X, y, sample_weight):\n",
    "    \"\"\"Rescale data so as to support sample_weight\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    sample_weight = np.asarray(sample_weight)\n",
    "    if sample_weight.ndim == 0:\n",
    "        sample_weight = np.full(n_samples, sample_weight, dtype=sample_weight.dtype)\n",
    "    sample_weight = np.sqrt(sample_weight)\n",
    "    sw_matrix = sparse.dia_matrix((sample_weight, 0), shape=(n_samples, n_samples))\n",
    "    X = safe_sparse_dot(sw_matrix, X)\n",
    "    y = safe_sparse_dot(sw_matrix, y)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "class SBAM (BaseOptimizer):\n",
    "    \"\"\"\n",
    "    Bayesian Regression with a Spike and Slab type prior Optimizer.\n",
    "    \n",
    "    Computes the most likely combination of the coefficient vector\n",
    "    and the masking vector using Bayesian inference to compute the \n",
    "    posterior distribution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fit_intercept : boolean, optional (default False)\n",
    "        Whether to calculate the intercept for this model. If set to false, no\n",
    "        intercept will be used in calculations.\n",
    "\n",
    "    normalize_columns : boolean, optional (default False)\n",
    "        Normalize the columns of x (the SINDy library terms) before regression\n",
    "        by dividing by the L2-norm. Note that the 'normalize' option in sklearn\n",
    "        is deprecated in sklearn versions >= 1.0 and will be removed.\n",
    "\n",
    "    copy_X : boolean, optional (default True)\n",
    "        If True, X will be copied; else, it may be overwritten.\n",
    "    \n",
    "\n",
    "    max_iter : int, optional (default 5000)\n",
    "        Maximum iterations of the optimization algorithm, i.e. the length of the Gibbs sampling chain.\n",
    "\n",
    "    burn_in : int, optional (default 1500)\n",
    "        Number of samples from the sampling chain to discard.\n",
    "    \n",
    "    verbose : boolean, optional (default False)\n",
    "        enables verbose\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "\n",
    "    coef_ : array, shape (n_features,) or (n_targets,n_features)\n",
    "        Coefficients vector.\n",
    "\n",
    "    ind_ : array, shape (n_features,) or (n_targets,n_features)\n",
    "        Vector of BOOL values indicating whether or not a feature is considered relevant in the sparse representation.\n",
    "\n",
    "    samples : list of length (n_targets) \n",
    "        Dictionaries containing pandas DataFrames of the samples generated by the Gibbs sampling algorithm.\n",
    "\n",
    "    ----------|    Model hyperparameters\n",
    "\n",
    "    a1=0.01 \n",
    "    a2=0.01 :\n",
    "        sigma2 ~ InvGamma(a1,a1)\n",
    "    \n",
    "    a=1. \n",
    "    b=1000. :\n",
    "        Theta ~ Beta(a,b)\n",
    "    \n",
    "    s=5. :\n",
    "        tau2 ~ InvGamma(0.5,s**2/2)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        max_iter=5000, \n",
    "        burn_in=1500,\n",
    "        normalize_columns=False, \n",
    "        fit_intercept=False, \n",
    "        initial_guess=None, \n",
    "        copy_X=True,\n",
    "        tol=1e-5, # apparently needed for compatibility?\n",
    "        alpha=None, # idk\n",
    "        verbose=False\n",
    "        ):\n",
    "\n",
    "        # super() calls a temporary version of the parent class\n",
    "        # this way we pass the init parameters to the class itself via inheritance\n",
    "        # without having to rewrite everything\n",
    "        super().__init__(max_iter, normalize_columns, fit_intercept, initial_guess, copy_X)\n",
    "\n",
    "        self.tol=tol\n",
    "        self.alpha=alpha\n",
    "        self.burn_in = burn_in\n",
    "        if self.max_iter <= 0:\n",
    "            raise ValueError(\"Max iteration must be > 0\")\n",
    "        self.verbose=verbose\n",
    "\n",
    "        # ---------------- PRIOR PARAMETERS\n",
    "\n",
    "        self.a1=0.01\n",
    "        self.a2=0.01\n",
    "        self.theta=0.5\n",
    "        self.a=1.\n",
    "        self.b=1000.\n",
    "        self.s=5.\n",
    "     \n",
    "\n",
    "    # WE WILL OVERRIDE THE FIT METHOD FROM BaseEstimator SINCE WE WANT DIFFERENT .ind_ attributes\n",
    "\n",
    "    def fit(self, x_, y, sample_weight=None, **reduce_kws):\n",
    "        \"\"\"\n",
    "        Fit the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_ : array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "\n",
    "        y : array-like, shape (n_samples,) or (n_samples, n_targets)\n",
    "            Target values\n",
    "\n",
    "        sample_weight : float or numpy array of shape (n_samples,), optional\n",
    "            Individual weights for each sample\n",
    "\n",
    "        reduce_kws : dict\n",
    "            Optional keyword arguments to pass to the _reduce method\n",
    "            (implemented by subclasses)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : returns an instance of self\n",
    "        \"\"\"\n",
    "\n",
    "        # ----------- rescaling part\n",
    "        x_, y = check_X_y(x_, y, accept_sparse=[], y_numeric=True, multi_output=True)\n",
    "\n",
    "        x, y, X_offset, y_offset, X_scale = _preprocess_data(\n",
    "            x_,\n",
    "            y,\n",
    "            fit_intercept=self.fit_intercept,\n",
    "            copy=self.copy_X,\n",
    "            sample_weight=sample_weight,\n",
    "        )\n",
    "\n",
    "        if sample_weight is not None:\n",
    "            x, y = _rescale_data(x, y, sample_weight)\n",
    "\n",
    "        # self.iters = 0\n",
    "\n",
    "\n",
    "        # ------------ preparing dimensions, if there is only one target (only one time derivative) then we set it (-1,1) shape\n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        coef_shape = (y.shape[1], x.shape[1])\n",
    "        self.ind_ = np.ones(coef_shape, dtype=bool)\n",
    "\n",
    "        # ----------- normalization\n",
    "        self.Theta_ = x # saving original theta\n",
    "        x_normed = np.copy(x)\n",
    "        if self.normalize_columns:\n",
    "            reg = 1 / np.linalg.norm(x, 2, axis=0)\n",
    "            x_normed = x * reg\n",
    "\n",
    "\n",
    "        # WHERE THE MAGIC HAPPENS\n",
    "\n",
    "        self._reduce(x_normed, y, **reduce_kws)\n",
    "        #self.ind_ = np.abs(self.coef_) > 1e-14 # WE WILL SET THIS IN THE REDUCE METHOD, its gonna be the most probable z vector\n",
    "\n",
    "        self.history_ = [self.coef_]\n",
    "\n",
    "        # Rescale coefficients to original units\n",
    "        if self.normalize_columns:\n",
    "            self.coef_ = np.multiply(reg, self.coef_)\n",
    "            if hasattr(self, \"coef_full_\"):\n",
    "                self.coef_full_ = np.multiply(reg, self.coef_full_)\n",
    "            for i in range(np.shape(self.history_)[0]):\n",
    "                self.history_[i] = np.multiply(reg, self.history_[i])\n",
    "\n",
    "        self._set_intercept(X_offset, y_offset, X_scale)\n",
    "        return self\n",
    "        \n",
    "\n",
    "    def sampling(self,y,X):\n",
    "\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        res = np.empty((self.max_iter,2*n_features + 3))\n",
    "\n",
    "        # # initialize the beta as least square regression\n",
    "        res[0,:n_features] = np.linalg.lstsq(X,y,rcond=None)[0]\n",
    "        # initialize the masking as zeros\n",
    "        res[0,n_features:2*n_features] = np.zeros(n_features)\n",
    "        # res[\"z\"][0] = (abs(res['beta'][0]) > 1e-3).astype(int) # try to initialize the masking as those that are higher than a certain threshold \n",
    "        # # initialize the sigma as the variance of the residuals\n",
    "        res[0,-3] = np.var(y - X @ res[0,:n_features])\n",
    "        # # initialize the tau2 as one and the theta as 0.5\n",
    "        res[0,-2] = 1.\n",
    "        res[0,-1] = 0.5\n",
    "\n",
    "        # compute only once\n",
    "        XtX = X.T @ X\n",
    "        Xty = X.T @ y\n",
    "\n",
    "        # ----------------- BEGIN SAMPLING\n",
    "        with alive_bar(self.max_iter-1,force_tty=True,bar='filling') as bar:\n",
    "\n",
    "            for i in range(1,self.max_iter):\n",
    "\n",
    "\n",
    "                # lets retrieve the previous values for easier coding\n",
    "                beta_prev = res[i-1,:n_features]\n",
    "                z_prev = res[i-1,n_features:2*n_features]\n",
    "                sigma2_prev = res[i-1,-3]\n",
    "                tau2_prev = res[i-1,-2]\n",
    "                theta_prev = res[i-1,-1]\n",
    "\n",
    "                # ------------------ LETS GO WITH THE CONDITIONALS\n",
    "\n",
    "                # sample theta from a Beta distribution\n",
    "                theta_new = beta.rvs(self.a + np.sum(z_prev),self.b+np.sum(1-z_prev))\n",
    "\n",
    "                # sample sigma2 from an inverse gamma\n",
    "                err = y - X @ beta_prev\n",
    "                scale = 1./(self.a2 + (err.T @ err)/2)\n",
    "\n",
    "                # sigma2_new = sigma2_prev\n",
    "                sigma2_new = 1./gamma.rvs(self.a1+n_samples/2,scale=scale)\n",
    "\n",
    "                # sample tau2 from an inverse gamma\n",
    "                scale = 1./((self.s**2)/2 + (beta_prev.T @ beta_prev)/(2*sigma2_new))\n",
    "\n",
    "                # try to fix tau2!\n",
    "                # tau2_new = tau2_prev\n",
    "                tau2_new = 1./gamma.rvs(0.5+0.5*np.sum(z_prev),scale=scale)\n",
    "\n",
    "                # sample new beta from a multivariate gaussian\n",
    "                covariance = np.linalg.inv(XtX/sigma2_new + np.eye(n_features)/(sigma2_new*tau2_new))\n",
    "                mean = covariance @ Xty /sigma2_new # is this right?\n",
    "                beta_new = multivariate_normal.rvs(mean = mean,cov=covariance)\n",
    "\n",
    "                # now we sample the zjs\n",
    "                # in random order\n",
    "                for j in np.random.permutation(n_features):\n",
    "                    \n",
    "                    # grab the current vector\n",
    "                    z0 = z_prev\n",
    "                    # set j to zero\n",
    "                    z0[j] = 0.\n",
    "                    # get the beta_{-j}\n",
    "                    bz0 = beta_new * z0\n",
    "\n",
    "                    # compute the u variables (one for each sample)\n",
    "                    xj = X[:,j] # the jth feature of each sample\n",
    "                    u = y - X @ bz0 \n",
    "                    cond_var = np.sum(xj**2) + 1./tau2_new\n",
    "\n",
    "                    # compute the chance parameter:\n",
    "                    # the probability of extracting zj = 0 is prop to (1-theta)\n",
    "                    # while of extracting zj=1 is (.....) mess \n",
    "                    # computing the logarithm of these (l0 and l1) means that the probability of extracting zj=1 is\n",
    "                    # xi = exp(l1)/(exp(l1)+exp(l0))\n",
    "                    # we can also write this as\n",
    "                    # xi = 1/(1+ exp(l0-l1))\n",
    "                    # this way we can check if exp(l0-l1) overflows and just call it xi = 0\n",
    "\n",
    "                    l0 = np.log(1-theta_new)\n",
    "                    l1 = np.log(theta_new) \\\n",
    "                        - 0.5 * np.log(tau2_new*sigma2_new) \\\n",
    "                        + (np.sum(xj*u)**2)/(2*sigma2_new*cond_var) \\\n",
    "                        + 0.5*np.log(sigma2_new/cond_var)\n",
    "\n",
    "                    el0_l1 = np.exp(l0-l1)\n",
    "                    if np.isinf(el0_l1):\n",
    "                        xi = 0\n",
    "                    else:\n",
    "                        xi = 1/(1+el0_l1)\n",
    "                    \n",
    "                    # extract the zj\n",
    "                    z_prev[j]=binom.rvs(1,xi)\n",
    "\n",
    "                # once we extracted all zj, store them:\n",
    "                z_new = z_prev\n",
    "\n",
    "                # update everything\n",
    "\n",
    "                # res[i,\"beta\"] = beta_new\n",
    "                res[i,:n_features] = beta_new*z_new\n",
    "                res[i,n_features:2*n_features] = z_new\n",
    "                res[i,-3] = sigma2_new\n",
    "                res[i,-2] = tau2_new\n",
    "                res[i,-1] = theta_new\n",
    "\n",
    "\n",
    "                if self.verbose:\n",
    "                    bar()\n",
    "\n",
    "            # ---------- END SAMPLING\n",
    "        \n",
    "        return res[self.burn_in:,:]\n",
    "\n",
    "\n",
    "    def _reduce(self,x,y,**sampling_kws):\n",
    "        \"\"\"\n",
    "        Reduce method to actually perform the minimization.\n",
    "        This method performs a Gibbs sampling of the joint probability distribution\n",
    "        Under the spike and slab prior method, and will return the coefficients\n",
    "        as the most probable one, given the most probable masking coefficient.\n",
    "        \"\"\"\n",
    "\n",
    "        n_samples, n_features = x.shape\n",
    "        n_targets = y.shape[1]\n",
    "        coef = np.zeros((n_targets,n_features))\n",
    "        ind = np.zeros((n_targets,n_features))\n",
    "\n",
    "\n",
    "        # hierarchical columns\n",
    "        columns = []\n",
    "\n",
    "        for i in range(n_features):\n",
    "            columns.append(('beta',str(i)))\n",
    "            columns.append(('z',str(i)))\n",
    "\n",
    "        columns.sort()\n",
    "\n",
    "        columns.append(('sigma2',))\n",
    "        columns.append(('tau2',))\n",
    "        columns.append(('theta',))\n",
    "\n",
    "        index = pd.MultiIndex.from_tuples(columns)\n",
    "\n",
    "        self.samples = []\n",
    "\n",
    "        for i in range(n_targets):\n",
    "\n",
    "            if self.verbose: \n",
    "                print(\"Sampling for target n# {}/{}...\".format(i,n_targets-1))\n",
    "\n",
    "            # we can call y[:,i] because it's been reshaped to (-1,1) even if n_targets=1\n",
    "            self.samples.append(pd.DataFrame(self.sampling(y[:,i],x,**sampling_kws),columns=index))\n",
    "\n",
    "            # mask the betas via the z value and calculate the mean\n",
    "            coef[i] = self.samples[i]['beta'][self.samples[i]['z']==1].mean()\n",
    "            ind[i] = self.samples[i]['z'].mode().values.astype(bool)\n",
    "            coef[i] = coef[i]*ind[i].astype(int)\n",
    "\n",
    "        self.coef_ = coef\n",
    "        self.ind_ = ind"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b48686ecf5c051869e44bca573c1817bb1844fb32a5df209df0a7813f2e01a7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
